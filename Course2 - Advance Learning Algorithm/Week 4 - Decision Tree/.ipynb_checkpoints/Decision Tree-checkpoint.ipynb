{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c5e2b2",
   "metadata": {},
   "source": [
    "# Decision Tree [source note](https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/blob/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/C2W4A1/C2_W4_Decision_Tree_with_Markdown.ipynb)\n",
    "> Implement decision tree from scratch and apply it to the task of classifying whether a mushroom is edible or poisonous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6013dc09",
   "metadata": {},
   "source": [
    "## 1 - Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1bfcbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e0f989",
   "metadata": {},
   "source": [
    "## 2 - Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2574f22",
   "metadata": {},
   "source": [
    "Suppose you are starting a company that grows and sells wild mushrooms.\n",
    "\n",
    "   - Since not all mushrooms are edible, you'd like to be able to tell whether a given mushroom is ***edible or poisonous based on it's physical attributes***\n",
    "   - You have some existing data that you can use for this task.\n",
    "   \n",
    "Can you use the data to help you identify which mushrooms can be sold safely?\n",
    "\n",
    "> Note: The dataset used is for illustrative purposes only. It is not meant to be a guide on identifying edible mushrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4faee7",
   "metadata": {},
   "source": [
    "## 3 - Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e00530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Brown cap  Tapering Stalk Shape  Solitary  Edible\n",
      "0          1                     1         1       1\n",
      "1          1                     0         1       1\n",
      "2          1                     0         0       0\n",
      "3          1                     0         0       0\n",
      "4          1                     1         1       1\n",
      "5          0                     1         1       0\n",
      "6          0                     0         0       0\n",
      "7          1                     0         1       1\n",
      "8          0                     1         0       1\n",
      "9          1                     0         0       0\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0])\n",
    "\n",
    "\n",
    "column_values = ['Brown cap', 'Tapering Stalk Shape', 'Solitary', 'Edible']\n",
    "df_data = pd.DataFrame(data = np.hstack((X_train, np.expand_dims(y_train, axis=1))),\n",
    "                       columns = column_values)\n",
    "\n",
    "print(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ebc77",
   "metadata": {},
   "source": [
    "## 4 - Decision Tree Refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a6c6b",
   "metadata": {},
   "source": [
    "In this practice lab, you will build a decision tree based on the dataset provided.\n",
    "\n",
    "- Recall that the steps for building a decision tree are as follows:\n",
    "    - Start with all examples at the root node\n",
    "    - Calculate information gain for splitting on all possible features, and pick the one with the highest information gain\n",
    "    - Split dataset according to the selected feature, and create left and right branches of the tree\n",
    "    - Keep repeating splitting process until stopping criteria is met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb450d3a",
   "metadata": {},
   "source": [
    "### 4.1 - Caculate entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b7f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    - compute entropy at a node\n",
    "    - entropy as a measure of impurity\n",
    "    Args: \n",
    "        y: label\n",
    "    Return: \n",
    "        entropy\n",
    "    \"\"\"\n",
    "    entropy = 0.0\n",
    "    \n",
    "    if y.all() == True or y.any() == False:\n",
    "        return entropy\n",
    "    \n",
    "    p1 = y.sum()/len(y)\n",
    "    \n",
    "    entropy = -p1 * np.log2(p1) - (1-p1) * np.log2(1 - p1)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0a90ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([1, 0, 1, 1])\n",
    "compute_entropy(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c284a",
   "metadata": {},
   "source": [
    "### 4.2 - Split dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4236b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Augs: \n",
    "        - \n",
    "        -\n",
    "        - \n",
    "    Returns:\n",
    "        - left_indices: np.array\n",
    "        - right_indeces: np.array\n",
    "    \"\"\"\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    \n",
    "    for i in node_indices:\n",
    "        if X[i, feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else: \n",
    "            right_indices.append(i)\n",
    "\n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e72301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left indices:  [0, 1, 2, 3, 4, 7, 9]\n",
      "Right indices:  [5, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Feel free to play around with these variables\n",
    "# The dataset only has three features, so this value can be 0 (Brown Cap), 1 (Tapering Stalk Shape) or 2 (Solitary)\n",
    "feature = 0\n",
    "\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature)\n",
    "\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9c2c1",
   "metadata": {},
   "source": [
    "### 4.3 - Caculate Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d03f126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gain(X, y, node_indices, feature):\\\n",
    "    # split dataset into left and right branch\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "\n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    information_gain = 0\n",
    "    \n",
    "    node_entropy = compute_entropy(y_node)\n",
    "    left_entropy = compute_entropy(y_left)\n",
    "    right_entropy = compute_entropy(y_right)\n",
    "    \n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    \n",
    "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
    "    \n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7bd55e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain from splitting the root on brown cap:  0.034851554559677034\n",
      "Information Gain from splitting the root on tapering stalk shape:  0.12451124978365313\n",
      "Information Gain from splitting the root on solitary:  0.2780719051126377\n"
     ]
    }
   ],
   "source": [
    "info_gain0 = compute_information_gain(X_train, y_train, root_indices, feature=0)\n",
    "print(\"Information Gain from splitting the root on brown cap: \", info_gain0)\n",
    "    \n",
    "info_gain1 = compute_information_gain(X_train, y_train, root_indices, feature=1)\n",
    "print(\"Information Gain from splitting the root on tapering stalk shape: \", info_gain1)\n",
    "\n",
    "info_gain2 = compute_information_gain(X_train, y_train, root_indices, feature=2)\n",
    "print(\"Information Gain from splitting the root on solitary: \", info_gain2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65befc",
   "metadata": {},
   "source": [
    "\n",
    "### 4.4 - Get best split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "526bafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(X, y, node_indices):   \n",
    "    \"\"\"\n",
    "    Returns the optimal feature and threshold value\n",
    "    to split the node data \n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "\n",
    "    Returns:\n",
    "        best_feature (int):     The index of the best feature to split\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Some useful variables\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    best_feature = -1\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    max_info_gain=0\n",
    "    for feature in range(num_features):\n",
    "        info_gain = compute_information_gain(X, y, node_indices, feature)\n",
    "        if info_gain > max_info_gain:\n",
    "            max_info_gain = info_gain\n",
    "            best_feature = feature\n",
    "                \n",
    "        \n",
    "    ### END CODE HERE ##    \n",
    "       \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b80f8e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature to split on: 2\n"
     ]
    }
   ],
   "source": [
    "best_feature = get_best_split(X_train, y_train, root_indices)\n",
    "print(\"Best feature to split on: %d\" % best_feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca4f7cd",
   "metadata": {},
   "source": [
    "## 5 - Building tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945edc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
